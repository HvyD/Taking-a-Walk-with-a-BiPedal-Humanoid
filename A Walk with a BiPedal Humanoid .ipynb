{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAi-GYM Bipedal_Walker_v2 Deep Q Network Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Using GPU\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    LongTensor = torch.LongTensor\n",
    "    FloatTensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting and Running Model with Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "SCREEN_WIDTH = 400\n",
    "SCREEN_LENGTH =600\n",
    "WINDOW_MAX_Y = 300\n",
    "WINDOW_MIN_Y = 200\n",
    "BUFFER_SIZE = 65536\n",
    "GAMMA = 0.999\n",
    "START_EXPLORE_RATIO = 0.7\n",
    "END_EXPLORE_RATIO = 0.05\n",
    "NUM_FEATURES = 24\n",
    "FALL_TIME = 30  \n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(NUM_FEATURES, 400)\n",
    "        self.hidden2 = nn.Linear(400, 300)\n",
    "        self.output = nn.Linear(300, 16)\n",
    "\n",
    "        # Weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                n = len(m.weight.data[1])\n",
    "                # \"Xavier\" initialization\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.0001)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.hidden1(x))\n",
    "        x = F.tanh(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "    def update(self,transition_buffer):\n",
    "        \n",
    "        transition_batch = transition_buffer.get_batch()\n",
    "        if transition_batch is None:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states = transition_batch\n",
    "        non_final_mask = [i for i, state in enumerate(next_states) if state is not None]\n",
    "        non_final_mask = LongTensor(non_final_mask)\n",
    "     \n",
    "        non_final_next_states = Variable(torch.cat([s for s in next_states if s is not None]).view(-1, NUM_FEATURES),\n",
    "                                         volatile=True)\n",
    "\n",
    "        states = Variable(torch.cat(list(states)).view(-1, NUM_FEATURES))\n",
    "        actions = Variable(torch.cat(list(actions)).view(-1, 1).type(LongTensor))\n",
    "        rewards = Variable(torch.cat(list(rewards)))\n",
    "\n",
    "        q_values = self.forward(states).gather(1, actions)\n",
    "\n",
    "        next_state_values = Variable(torch.zeros(32).type(FloatTensor))\n",
    "\n",
    "        next_state_values[non_final_mask] = self.forward(non_final_next_states).max(1)[0]\n",
    "\n",
    "        next_state_values.volatile = False\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + rewards\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(q_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "class TransitionBuffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def push(self,transition):\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > BUFFER_SIZE:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def get_batch(self):\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "            # Transposing list of lists\n",
    "            return list(zip(*batch))   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_action(model,state, explore_ratio, randomization=True):\n",
    "    chance = random.random()\n",
    "    if chance < explore_ratio and randomization:\n",
    "        return LongTensor([random.randint(0, 15)])\n",
    "    else:\n",
    "        q_values = model(state)\n",
    "\n",
    "        return q_values.max(0)[1].data\n",
    "\n",
    "\n",
    "def get_action_vec(action_ind):\n",
    "    action_vec = np.array([int(bit) for bit in '{0:04b}'.format(action_ind)])\n",
    "    return action_vec*2 - 1\n",
    "\n",
    "\n",
    "def get_decay_ratio():\n",
    "    return math.pow(END_EXPLORE_RATIO/START_EXPLORE_RATIO, 1.5/NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0 , steps =  119 , total reward: -19.0581164122 , steps_avg: 119.0 , reward_avg: -19.0581164122 , distance traveled: 13.5595203378 , average speed: 0.113945549057 , explore ratio: 0.7\n",
      "Episode 1 , steps =  78 , total reward: -7.50757806337 , steps_avg: 98.5 , reward_avg: -13.2828472378 , distance traveled: 3.06957130428 , average speed: 0.0393534782601 , explore ratio: 0.6972344672149957\n",
      "Episode 2 , steps =  66 , total reward: -11.3184605069 , steps_avg: 87.6666666667 , reward_avg: -12.6280516608 , distance traveled: -0.572279317807 , average speed: -0.00867089875466 , explore ratio: 0.6944798603893985\n",
      "Episode 3 , steps =  67 , total reward: -1.71130873544 , steps_avg: 82.5 , reward_avg: -9.89886592949 , distance traveled: 9.69376056612 , average speed: 0.144682993524 , explore ratio: 0.6917361363573526\n",
      "Episode 4 , steps =  120 , total reward: -26.0880504545 , steps_avg: 90.0 , reward_avg: -13.1367028345 , distance traveled: -7.90419004505 , average speed: -0.0658682503754 , explore ratio: 0.6890032521235406\n",
      "Episode 5 , steps =  61 , total reward: -11.0832866591 , steps_avg: 85.1666666667 , reward_avg: -12.7944668052 , distance traveled: 1.20933923911 , average speed: 0.0198252334281 , explore ratio: 0.6862811648625089\n",
      "Episode 6 , steps =  47 , total reward: -7.18181689152 , steps_avg: 79.7142857143 , reward_avg: -11.9926596747 , distance traveled: 1.67715535417 , average speed: 0.0356841564717 , explore ratio: 0.6835698319179973\n",
      "Episode 7 , steps =  852 , total reward: -113.789441365 , steps_avg: 176.25 , reward_avg: -24.717257386 , distance traveled: -28.3734671248 , average speed: -0.033302191461 , explore ratio: 0.68086921080227\n",
      "Episode 8 , steps =  109 , total reward: -19.4821205616 , steps_avg: 168.777777778 , reward_avg: -24.1355755166 , distance traveled: 8.85059555199 , average speed: 0.0811981243302 , explore ratio: 0.6781792591954505\n",
      "Episode 9 , steps =  108 , total reward: -3.93186347722 , steps_avg: 162.7 , reward_avg: -22.1152043126 , distance traveled: 15.4370592486 , average speed: 0.142935733784 , explore ratio: 0.6754999349448578\n",
      "Episode 10 , steps =  1701 , total reward: -142.918843586 , steps_avg: 302.545454545 , reward_avg: -33.0973533375 , distance traveled: 72.517364372 , average speed: 0.042632195398 , explore ratio: 0.672831196064346\n",
      "Episode 11 , steps =  87 , total reward: -8.66017512665 , steps_avg: 284.583333333 , reward_avg: -31.0609218199 , distance traveled: 3.84561909685 , average speed: 0.0442025183546 , explore ratio: 0.6701730007336466\n"
     ]
    }
   ],
   "source": [
    "%run ACPreTrain.py\n",
    "env = gym.make('BipedalWalker-v2').unwrapped\n",
    "env.reset()\n",
    "model = DQN()\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "transition_buffer = TransitionBuffer()\n",
    "\n",
    "reward_his = np.zeros(NUM_EPISODES)\n",
    "steps_his = np.zeros(NUM_EPISODES)\n",
    "distance_his = np.zeros(NUM_EPISODES)\n",
    "velocity_his = np.zeros(NUM_EPISODES)\n",
    "\n",
    "min_max_states = np.zeros((NUM_FEATURES, 2))\n",
    "\n",
    "explore_ratio = START_EXPLORE_RATIO\n",
    "explore_decay_ratio = get_decay_ratio()\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "    action_vec = env.action_space.sample()\n",
    "\n",
    "    current_state = FloatTensor(np.zeros(NUM_FEATURES))\n",
    "\n",
    "    for i in count():\n",
    "        #env.render(mode='rgb_array')\n",
    "\n",
    "        if i < FALL_TIME:\n",
    "               action_ind = 8\n",
    "\n",
    "        else:\n",
    "            randomization = bool(np.mod(episode, 50))\n",
    "            action_ind = get_action(model, Variable(current_state, volatile=True), explore_ratio, randomization)\n",
    "            action_vec = get_action_vec(int(action_ind.cpu().numpy()))\n",
    "\n",
    "        obs, reward, done, info = env.step(action_vec)\n",
    "\n",
    "        distance_his[episode] += obs[2]\n",
    "\n",
    "        if done is False:\n",
    "            next_state = FloatTensor(obs[:NUM_FEATURES])\n",
    "            reward_his[episode] += reward\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        if i >= FALL_TIME:\n",
    "            transition_buffer.push([current_state, action_ind, FloatTensor([reward]), next_state])\n",
    "\n",
    "        current_state = next_state\n",
    "        \n",
    "        #update model\n",
    "        model.update(transition_buffer)\n",
    "\n",
    "        if done is True:\n",
    "            steps_his[episode] = i\n",
    "            velocity_his[episode] = distance_his[episode]/i\n",
    "            print(\"Episode\", episode, \", steps = \", i,\n",
    "                    \", total reward:\", reward_his[episode],\n",
    "                    \", steps_avg:\", np.mean(steps_his[:episode+1]),\n",
    "                    \", reward_avg:\", np.mean(reward_his[:episode+1]),\n",
    "                    \", distance traveled:\", distance_his[episode],\n",
    "                    \", average speed:\", velocity_his[episode],\n",
    "                    \", explore ratio:\", explore_ratio)\n",
    "            break\n",
    "\n",
    "    if explore_ratio > END_EXPLORE_RATIO:\n",
    "           explore_ratio = explore_ratio*explore_decay_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visiualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"distance traveled\")\n",
    "plt.xlabel(\"episode id\")\n",
    "plt.plot(np.arange(0, NUM_EPISODES, 1), distance_his)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"avg velocity\")\n",
    "plt.xlabel(\"episode id\")\n",
    "plt.plot(np.arange(0, NUM_EPISODES, 1), velocity_his)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
