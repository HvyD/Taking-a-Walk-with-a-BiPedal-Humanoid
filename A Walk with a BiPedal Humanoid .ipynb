{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting and Running Model with Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 2000\n",
    "SCREEN_WIDTH = 400\n",
    "SCREEN_LENGTH =600\n",
    "WINDOW_MAX_Y = 300\n",
    "WINDOW_MIN_Y = 200\n",
    "BUFFER_SIZE = 65536\n",
    "GAMMA = 0.999\n",
    "START_EXPLORE_RATIO = 0.7\n",
    "END_EXPLORE_RATIO = 0.05\n",
    "NUM_FEATURES = 24\n",
    "FALL_TIME = 30\n",
    "\n",
    "# Check for GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Using GPU\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    LongTensor = torch.LongTensor\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(NUM_FEATURES, 400)\n",
    "        self.hidden2 = nn.Linear(400, 300)\n",
    "        self.output = nn.Linear(300, 16)\n",
    "\n",
    "        # Weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # 'n' is number of inputs to each neuron\n",
    "                n = len(m.weight.data[1])\n",
    "                # \"Xavier\" initialization\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.0001)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.hidden1(x))\n",
    "        x = F.tanh(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "    def update(self,transition_buffer):\n",
    "        \n",
    "        transition_batch = transition_buffer.get_batch()\n",
    "        if transition_batch is None:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states = transition_batch\n",
    "        non_final_mask = [i for i, state in enumerate(next_states) if state is not None]\n",
    "        non_final_mask = LongTensor(non_final_mask)\n",
    "     \n",
    "        non_final_next_states = Variable(torch.cat([s for s in next_states if s is not None]).view(-1, NUM_FEATURES),\n",
    "                                         volatile=True)\n",
    "\n",
    "        states = Variable(torch.cat(list(states)).view(-1, NUM_FEATURES))\n",
    "        actions = Variable(torch.cat(list(actions)).view(-1, 1).type(LongTensor))\n",
    "        rewards = Variable(torch.cat(list(rewards)))\n",
    "\n",
    "        q_values = self.forward(states).gather(1, actions)\n",
    "\n",
    "        next_state_values = Variable(torch.zeros(32).type(FloatTensor))\n",
    "\n",
    "        next_state_values[non_final_mask] = self.forward(non_final_next_states).max(1)[0]\n",
    "\n",
    "        next_state_values.volatile = False\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + rewards\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(q_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "class TransitionBuffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def push(self,transition):\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > BUFFER_SIZE:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def get_batch(self):\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "            # Transposing list of lists\n",
    "            return list(zip(*batch))   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0 , steps =  100 , total reward: -5.08322124353 , steps_avg: 100.0 , reward_avg: -5.08322124353 , distance traveled: 15.2025455569 , average speed: 0.152025455569 , explore ratio: 0.7\n",
      "Episode 1 , steps =  106 , total reward: -9.29547454337 , steps_avg: 103.0 , reward_avg: -7.18934789345 , distance traveled: 11.4508545123 , average speed: 0.108026929361 , explore ratio: 0.6986158651580259\n",
      "Episode 2 , steps =  56 , total reward: -11.2519507804 , steps_avg: 87.3333333333 , reward_avg: -8.54354885575 , distance traveled: -0.296580379754 , average speed: -0.0052960782099 , explore ratio: 0.6972344672149957\n",
      "Episode 3 , steps =  61 , total reward: -3.99612889055 , steps_avg: 80.75 , reward_avg: -7.40669386445 , distance traveled: 5.77588205516 , average speed: 0.0946865910683 , explore ratio: 0.6958558007591421\n",
      "Episode 4 , steps =  761 , total reward: -104.711999893 , steps_avg: 216.8 , reward_avg: -26.8677550701 , distance traveled: -28.7522769443 , average speed: -0.0377822298873 , explore ratio: 0.6944798603893986\n",
      "Episode 5 , steps =  118 , total reward: -16.356500938 , steps_avg: 200.333333333 , reward_avg: -25.1158793814 , distance traveled: 0.0119844605494 , average speed: 0.000101563224995 , explore ratio: 0.6931066407153782\n",
      "Episode 6 , steps =  53 , total reward: -5.79726393937 , steps_avg: 179.285714286 , reward_avg: -22.3560771754 , distance traveled: 3.60064759278 , average speed: 0.0679367470336 , explore ratio: 0.6917361363573528\n",
      "Episode 7 , steps =  107 , total reward: -5.65238471813 , steps_avg: 170.25 , reward_avg: -20.2681156183 , distance traveled: 11.2333176911 , average speed: 0.104984277487 , explore ratio: 0.6903683419462316\n",
      "Episode 8 , steps =  82 , total reward: -15.0006176804 , steps_avg: 160.444444444 , reward_avg: -19.6828380696 , distance traveled: 1.48174917346 , average speed: 0.0180701118714 , explore ratio: 0.6890032521235407\n",
      "Episode 9 , steps =  935 , total reward: -89.9796407172 , steps_avg: 237.9 , reward_avg: -26.7125183344 , distance traveled: 30.0596513792 , average speed: 0.0321493597639 , explore ratio: 0.6876408615414012\n",
      "Episode 10 , steps =  146 , total reward: -27.8665434812 , steps_avg: 229.545454545 , reward_avg: -26.8174297113 , distance traveled: -7.862665424 , average speed: -0.0538538727671 , explore ratio: 0.6862811648625091\n"
     ]
    }
   ],
   "source": [
    "def get_action(model,state, explore_ratio, randomization=True):\n",
    "    chance = random.random()\n",
    "    if chance < explore_ratio and randomization:\n",
    "        return LongTensor([random.randint(0, 15)])\n",
    "    else:\n",
    "        q_values = model(state)\n",
    "\n",
    "        return q_values.max(0)[1].data\n",
    "\n",
    "\n",
    "def get_action_vec(action_ind):\n",
    "    action_vec = np.array([int(bit) for bit in '{0:04b}'.format(action_ind)])\n",
    "    return action_vec*2 - 1\n",
    "\n",
    "\n",
    "def get_decay_ratio():\n",
    "    return math.pow(END_EXPLORE_RATIO/START_EXPLORE_RATIO, 1.5/NUM_EPISODES)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('BipedalWalker-v2').unwrapped\n",
    "    env.reset()\n",
    "    model = DQN()\n",
    "\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    transition_buffer = TransitionBuffer()\n",
    "\n",
    "    reward_his = np.zeros(NUM_EPISODES)\n",
    "    steps_his = np.zeros(NUM_EPISODES)\n",
    "    distance_his = np.zeros(NUM_EPISODES)\n",
    "    velocity_his = np.zeros(NUM_EPISODES)\n",
    "\n",
    "    min_max_states = np.zeros((NUM_FEATURES, 2))\n",
    "\n",
    "    explore_ratio = START_EXPLORE_RATIO\n",
    "    explore_decay_ratio = get_decay_ratio()\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        env.reset()\n",
    "        action_vec = env.action_space.sample()\n",
    "\n",
    "\n",
    "        current_state = FloatTensor(np.zeros(NUM_FEATURES))\n",
    "\n",
    "        for i in count():\n",
    "        #for i in range(100):\n",
    "            #Rendering screen\n",
    "            env.render(mode='rgb_array')\n",
    "\n",
    "            if i < FALL_TIME:\n",
    "                action_ind = 8\n",
    "\n",
    "            else:\n",
    "                randomization = bool(np.mod(episode, 50))\n",
    "                action_ind = get_action(model, Variable(current_state, volatile=True), explore_ratio, randomization)\n",
    "                action_vec = get_action_vec(int(action_ind.cpu().numpy()))\n",
    "            #Executing step \n",
    "            obs, reward, done, info = env.step(action_vec)\n",
    "\n",
    "            #Saving velocity\n",
    "            distance_his[episode] += obs[2]\n",
    "\n",
    "            if done is False:\n",
    "                next_state = FloatTensor(obs[:NUM_FEATURES])\n",
    "                reward_his[episode] += reward\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            #Storing transition into transition buffer\n",
    "            if i >= FALL_TIME:\n",
    "                transition_buffer.push([current_state, action_ind, FloatTensor([reward]), next_state])\n",
    "\n",
    "            #Updating states\n",
    "            current_state = next_state\n",
    "\n",
    "            #Updating model\n",
    "            model.update(transition_buffer)\n",
    "\n",
    "            if done is True:\n",
    "                steps_his[episode] = i\n",
    "                velocity_his[episode] = distance_his[episode]/i\n",
    "                print(\"Episode\", episode, \", steps = \", i,\n",
    "                      \", total reward:\", reward_his[episode],\n",
    "                      \", steps_avg:\", np.mean(steps_his[:episode+1]),\n",
    "                      \", reward_avg:\", np.mean(reward_his[:episode+1]),\n",
    "                      \", distance traveled:\", distance_his[episode],\n",
    "                      \", average speed:\", velocity_his[episode],\n",
    "                      \", explore ratio:\", explore_ratio)\n",
    "                break\n",
    "\n",
    "        if explore_ratio > END_EXPLORE_RATIO:\n",
    "            explore_ratio = explore_ratio*explore_decay_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visiualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"distance traveled\")\n",
    "plt.xlabel(\"episode id\")\n",
    "plt.plot(np.arange(0, NUM_EPISODES, 1), distance_his)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"avg velocity\")\n",
    "plt.xlabel(\"episode id\")\n",
    "plt.plot(np.arange(0, NUM_EPISODES, 1), velocity_his)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
